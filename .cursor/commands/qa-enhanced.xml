<?xml version="1.0" encoding="UTF-8"?>
<testing-prompt version="2.0">
    <metadata>
        <purpose>Advanced Comprehensive Test Coverage and Quality Validation</purpose>
        <test-types>
            <type>Unit Tests (Vitest + React Testing Library)</type>
            <type>E2E Tests (Playwright)</type>
            <type>Integration Tests</type>
            <type>Visual Regression Tests</type>
            <type>Accessibility Tests</type>
            <type>Performance Tests</type>
        </test-types>
        <project-context>
            <framework>Vitest</framework>
            <library>React Testing Library</library>
            <e2e>Playwright</e2e>
            <co-location>true</co-location>
            <selector>data-testid</selector>
        </project-context>
    </metadata>

    <testing-workflow>
        <!-- PHASE 1: PRE-ANALYSIS -->
        <step name="pre-test-analysis">
            <objective>Understand code changes and testing requirements</objective>
            <actions>
                <action>Identify modified files and their types</action>
                <action>Detect new components/functions requiring tests</action>
                <action>Analyze complexity of changes</action>
                <action>Review code review findings for testing implications</action>
                <action>Check for breaking changes</action>
                <action>Identify affected user flows</action>
            </actions>
        </step>

        <!-- PHASE 2: EXISTING TEST VALIDATION -->
        <step name="existing-test-validation">
            <objective>Verify current test suite integrity</objective>
            <actions>
                <action priority="critical">Run full unit test suite (Vitest)</action>
                <action priority="critical">Run E2E test suite (Playwright)</action>
                <action priority="high">Identify failing tests</action>
                <action priority="high">Analyze test failures with root cause</action>
                <action priority="high">Detect flaky tests</action>
                <action priority="medium">Measure test execution time</action>
                <action priority="medium">Check for test isolation issues</action>
                <action priority="low">Identify slow tests (> 5s unit, > 30s E2E)</action>
            </actions>
        </step>

        <!-- PHASE 3: TEST FAILURE ANALYSIS -->
        <test-failure-handling>
            <failure-tracking>
                <fields>
                    <field>Test Name</field>
                    <field>Test Type (Unit/Integration/E2E)</field>
                    <field>File Location</field>
                    <field>Error Type</field>
                    <field>Error Message</field>
                    <field>Stack Trace</field>
                    <field>Failure Location (line number)</field>
                    <field>Potential Root Cause</field>
                    <field>Is Flaky? (Yes/No/Suspected)</field>
                    <field>Related Code Changes</field>
                    <field>Recommended Fix</field>
                    <field>Priority (Critical/High/Medium/Low)</field>
                </fields>
            </failure-tracking>

            <failure-classification>
                <type name="regression">
                    <description>Previously passing test now fails</description>
                    <priority>Critical</priority>
                    <action>Fix immediately</action>
                </type>
                <type name="new-feature-incomplete">
                    <description>Test for new feature fails</description>
                    <priority>High</priority>
                    <action>Complete implementation or adjust test</action>
                </type>
                <type name="flaky-test">
                    <description>Test passes/fails intermittently</description>
                    <priority>High</priority>
                    <action>Stabilize or temporarily skip with ticket</action>
                </type>
                <type name="environment-issue">
                    <description>Test fails due to environment setup</description>
                    <priority>Medium</priority>
                    <action>Fix test setup/teardown</action>
                </type>
                <type name="test-bug">
                    <description>Test itself has a bug</description>
                    <priority>Medium</priority>
                    <action>Fix test code</action>
                </type>
            </failure-classification>

            <flaky-test-detection>
                <indicators>
                    <indicator>Test passes locally but fails in CI</indicator>
                    <indicator>Test depends on timing (setTimeout, animations)</indicator>
                    <indicator>Test depends on external state</indicator>
                    <indicator>Test uses real dates/times without mocking</indicator>
                    <indicator>Test has race conditions</indicator>
                    <indicator>Test depends on test execution order</indicator>
                </indicators>
                <recommendations>
                    <recommendation>Add proper waitFor/findBy queries</recommendation>
                    <recommendation>Mock timers (vi.useFakeTimers())</recommendation>
                    <recommendation>Use deterministic data</recommendation>
                    <recommendation>Ensure test isolation</recommendation>
                    <recommendation>Add retries for E2E tests only</recommendation>
                </recommendations>
            </flaky-test-detection>

            <failure-resolution-strategy>
                <approach>Systematic Sequential Fix</approach>
                <steps>
                    <step priority="1">Fix critical regressions first</step>
                    <step priority="2">Fix high-priority new feature tests</step>
                    <step priority="3">Stabilize or skip flaky tests</step>
                    <step priority="4">Fix medium priority issues</step>
                    <step priority="5">Validate after each fix</step>
                    <step priority="6">Run full regression suite</step>
                </steps>
            </failure-resolution-strategy>
        </test-failure-handling>

        <!-- PHASE 4: COVERAGE ANALYSIS -->
        <coverage-analysis>
            <metrics>
                <metric name="line-coverage">
                    <target>80%</target>
                    <critical-threshold>60%</critical-threshold>
                </metric>
                <metric name="branch-coverage">
                    <target>75%</target>
                    <critical-threshold>55%</critical-threshold>
                </metric>
                <metric name="function-coverage">
                    <target>90%</target>
                    <critical-threshold>70%</critical-threshold>
                </metric>
                <metric name="statement-coverage">
                    <target>80%</target>
                    <critical-threshold>60%</critical-threshold>
                </metric>
            </metrics>

            <analysis-areas>
                <area name="new-code">
                    <description>Code added in this change</description>
                    <requirement>Must have tests before merge</requirement>
                    <target-coverage>100%</target-coverage>
                </area>
                <area name="modified-code">
                    <description>Existing code that was changed</description>
                    <requirement>Tests must be updated</requirement>
                    <target-coverage>Maintain or improve existing</target-coverage>
                </area>
                <area name="critical-paths">
                    <description>Authentication, payments, data mutations</description>
                    <requirement>Must have comprehensive tests</requirement>
                    <target-coverage>100%</target-coverage>
                </area>
                <area name="utility-functions">
                    <description>Pure functions and helpers</description>
                    <requirement>Should have unit tests</requirement>
                    <target-coverage>100%</target-coverage>
                </area>
            </analysis-areas>

            <gap-identification>
                <check>Untested functions/methods</check>
                <check>Untested branches (if/else, switch cases)</check>
                <check>Untested error scenarios</check>
                <check>Untested edge cases</check>
                <check>Untested user interactions</check>
                <check>Untested async operations</check>
                <check>Untested component states</check>
                <check>Untested props variations</check>
            </gap-identification>
        </coverage-analysis>

        <!-- PHASE 5: TEST QUALITY ASSESSMENT -->
        <test-quality-assessment>
            <quality-indicators>
                <indicator name="test-isolation">
                    <description>Tests don't depend on each other</description>
                    <check>No shared state between tests</check>
                    <check>Proper setup/teardown</check>
                </indicator>
                <indicator name="test-clarity">
                    <description>Tests are readable and maintainable</description>
                    <check>Descriptive test names</check>
                    <check>Clear arrange-act-assert structure</check>
                    <check>Minimal complexity</check>
                </indicator>
                <indicator name="test-coverage-quality">
                    <description>Tests verify behavior, not implementation</description>
                    <check>Testing user-facing behavior</check>
                    <check>Not testing implementation details</check>
                    <check>Comprehensive assertions</check>
                </indicator>
                <indicator name="test-maintainability">
                    <description>Tests are easy to maintain</description>
                    <check>No brittle selectors</check>
                    <check>Proper use of data-testid</check>
                    <check>Minimal duplication</check>
                </indicator>
            </quality-indicators>

            <anti-patterns>
                <anti-pattern name="testing-implementation-details">
                    <example>Testing internal state instead of output</example>
                    <problem>Tests break when refactoring</problem>
                    <solution>Test user-facing behavior</solution>
                </anti-pattern>
                <anti-pattern name="shallow-assertions">
                    <example>Only checking that component renders</example>
                    <problem>Doesn't catch functional bugs</problem>
                    <solution>Assert on actual behavior and output</solution>
                </anti-pattern>
                <anti-pattern name="test-interdependence">
                    <example>Tests depending on execution order</example>
                    <problem>Flaky tests, hard to debug</problem>
                    <solution>Ensure test isolation</solution>
                </anti-pattern>
                <anti-pattern name="missing-error-tests">
                    <example>Only testing happy path</example>
                    <problem>Error scenarios uncovered</problem>
                    <solution>Test failure modes and edge cases</solution>
                </anti-pattern>
            </anti-patterns>
        </test-quality-assessment>

        <!-- PHASE 6: NEW TEST RECOMMENDATIONS -->
        <new-test-creation>
            <test-types>
                <type name="unit-tests">
                    <when-to-create>
                        <scenario>New component created</scenario>
                        <scenario>New hook created</scenario>
                        <scenario>New utility function added</scenario>
                        <scenario>Existing component significantly modified</scenario>
                    </when-to-create>
                    <coverage-goals>
                        <goal>100% function coverage for pure functions</goal>
                        <goal>All component states tested</goal>
                        <goal>All props variations tested</goal>
                        <goal>Edge case testing (empty, null, undefined, extremes)</goal>
                        <goal>Error scenario testing</goal>
                        <goal>Input validation testing</goal>
                    </coverage-goals>
                    <test-structure>
                        <section name="render-tests">
                            <test>Component renders without crashing</test>
                            <test>Renders with required props</test>
                            <test>Renders with optional props</test>
                            <test>Renders different states</test>
                        </section>
                        <section name="interaction-tests">
                            <test>Click handlers work</test>
                            <test>Form submissions work</test>
                            <test>Keyboard interactions work</test>
                        </section>
                        <section name="state-tests">
                            <test>State updates correctly</test>
                            <test>Side effects trigger properly</test>
                        </section>
                        <section name="edge-case-tests">
                            <test>Handles loading states</test>
                            <test>Handles error states</test>
                            <test>Handles empty/null data</test>
                        </section>
                    </test-structure>
                    <test-attributes>
                        <attribute>Isolation (no external dependencies)</attribute>
                        <attribute>Repeatability (same input = same output)</attribute>
                        <attribute>Independence (can run in any order)</attribute>
                        <attribute>Fast (< 1 second per test)</attribute>
                        <attribute>Focused (one concept per test)</attribute>
                    </test-attributes>
                </type>

                <type name="integration-tests">
                    <when-to-create>
                        <scenario>Multiple components interact</scenario>
                        <scenario>Hook interacts with external service</scenario>
                        <scenario>Data flows through multiple layers</scenario>
                    </when-to-create>
                    <coverage-goals>
                        <goal>Component integration testing</goal>
                        <goal>Context providers with consumers</goal>
                        <goal>Custom hooks with API calls</goal>
                        <goal>Form flows with validation</goal>
                    </coverage-goals>
                </type>

                <type name="e2e-tests">
                    <when-applicable>Critical user flows and journeys</when-applicable>
                    <when-to-create>
                        <scenario>New user flow added</scenario>
                        <scenario>Critical path modified</scenario>
                        <scenario>Authentication flow</scenario>
                        <scenario>Payment/transaction flow</scenario>
                        <scenario>Multi-step forms</scenario>
                    </when-to-create>
                    <coverage-goals>
                        <goal>User flow validation (happy path)</goal>
                        <goal>User flow validation (error paths)</goal>
                        <goal>Cross-browser compatibility</goal>
                        <goal>Mobile responsiveness</goal>
                        <goal>Interactive element testing</goal>
                        <goal>Navigation flows</goal>
                        <goal>Authentication flows</goal>
                    </coverage-goals>
                    <best-practices>
                        <practice>Test real user scenarios</practice>
                        <practice>Use page object pattern</practice>
                        <practice>Add appropriate waits (waitForSelector)</practice>
                        <practice>Use data-testid for stability</practice>
                        <practice>Take screenshots on failure</practice>
                        <practice>Test across viewports</practice>
                    </best-practices>
                </type>

                <type name="accessibility-tests">
                    <when-to-create>
                        <scenario>New interactive component</scenario>
                        <scenario>Form elements added</scenario>
                        <scenario>Navigation changes</scenario>
                    </when-to-create>
                    <coverage-goals>
                        <goal>Keyboard navigation works</goal>
                        <goal>Screen reader compatibility</goal>
                        <goal>ARIA attributes correct</goal>
                        <goal>Focus management proper</goal>
                        <goal>Color contrast sufficient</goal>
                    </coverage-goals>
                    <tools>
                        <tool>axe-core for automated a11y testing</tool>
                        <tool>jest-axe for unit test integration</tool>
                        <tool>Playwright accessibility tests</tool>
                    </tools>
                </type>

                <type name="performance-tests">
                    <when-to-create>
                        <scenario>New list/table with large datasets</scenario>
                        <scenario>Heavy computation added</scenario>
                        <scenario>Firestore query optimization</scenario>
                    </when-to-create>
                    <metrics>
                        <metric>Render time < 100ms</metric>
                        <metric>Interaction responsiveness < 50ms</metric>
                        <metric>Bundle size impact</metric>
                        <metric>Memory usage</metric>
                    </metrics>
                </type>

                <type name="visual-regression-tests">
                    <when-to-create>
                        <scenario>UI component changes</scenario>
                        <scenario>CSS/Tailwind modifications</scenario>
                        <scenario>Responsive design changes</scenario>
                    </when-to-create>
                    <tool>Playwright screenshot comparison</tool>
                </type>
            </test-types>
        </new-test-creation>

        <!-- PHASE 7: TEST PRIORITIZATION -->
        <test-prioritization>
            <priority-levels>
                <level name="critical" order="1">
                    <tests>
                        <test>Authentication flows</test>
                        <test>Payment/transaction processing</test>
                        <test>Data mutations (create/update/delete)</test>
                        <test>Security-critical operations</test>
                        <test>User registration/login</test>
                    </tests>
                    <requirement>Must have comprehensive tests</requirement>
                </level>
                <level name="high" order="2">
                    <tests>
                        <test>Main user flows</test>
                        <test>Core business logic</test>
                        <test>Data validation</test>
                        <test>Error handling</test>
                        <test>API integrations</test>
                    </tests>
                    <requirement>Should have thorough tests</requirement>
                </level>
                <level name="medium" order="3">
                    <tests>
                        <test>UI components</test>
                        <test>Utility functions</test>
                        <test>Helper methods</test>
                        <test>Secondary features</test>
                    </tests>
                    <requirement>Should have basic tests</requirement>
                </level>
                <level name="low" order="4">
                    <tests>
                        <test>Static content</test>
                        <test>Simple display components</test>
                        <test>Configuration files</test>
                    </tests>
                    <requirement>Nice to have tests</requirement>
                </level>
            </priority-levels>
        </test-prioritization>

        <!-- PHASE 8: REPORTING -->
        <reporting>
            <output-format>Markdown with tables and metrics</output-format>
            <report-sections>
                <section name="executive-summary">
                    <content>Overall test health score</content>
                    <content>Pass/fail statistics</content>
                    <content>Coverage metrics</content>
                    <content>Top concerns</content>
                </section>
                <section name="existing-test-failures">
                    <content>Detailed failure analysis</content>
                    <content>Root cause identification</content>
                    <content>Fix recommendations</content>
                </section>
                <section name="flaky-tests">
                    <content>Identified flaky tests</content>
                    <content>Flakiness indicators</content>
                    <content>Stabilization recommendations</content>
                </section>
                <section name="coverage-analysis">
                    <content>Current coverage metrics</content>
                    <content>Coverage change from baseline</content>
                    <content>Uncovered critical code</content>
                    <content>Coverage gaps</content>
                </section>
                <section name="new-tests-recommended">
                    <content>Recommended unit tests</content>
                    <content>Recommended E2E tests</content>
                    <content>Recommended integration tests</content>
                    <content>Priority and effort estimates</content>
                </section>
                <section name="test-quality-assessment">
                    <content>Test quality score</content>
                    <content>Anti-patterns detected</content>
                    <content>Improvement suggestions</content>
                </section>
                <section name="action-plan">
                    <content>Prioritized fix list</content>
                    <content>Quick wins</content>
                    <content>Long-term improvements</content>
                </section>
            </report-sections>
        </reporting>
    </testing-workflow>

    <!-- TEST HEALTH SCORING -->
    <test-health-scoring>
        <metrics>
            <metric name="test-pass-rate" weight="35">
                <calculation>Passing tests / Total tests * 100</calculation>
                <target>100%</target>
            </metric>
            <metric name="coverage" weight="25">
                <calculation>Average of line/branch/function coverage</calculation>
                <target>80%</target>
            </metric>
            <metric name="test-quality" weight="20">
                <calculation>Based on anti-pattern detection</calculation>
                <target>90%</target>
            </metric>
            <metric name="test-speed" weight="10">
                <calculation>Average test execution time</calculation>
                <target>Fast (< 1s unit, < 30s E2E)</target>
            </metric>
            <metric name="flakiness" weight="10">
                <calculation>100 - (flaky tests / total tests * 100)</calculation>
                <target>0 flaky tests</target>
            </metric>
        </metrics>
        <scoring>
            <range min="90" max="100">Excellent</range>
            <range min="75" max="89">Good</range>
            <range min="60" max="74">Acceptable</range>
            <range min="40" max="59">Needs Attention</range>
            <range min="0" max="39">Critical State</range>
        </scoring>
    </test-health-scoring>

    <!-- EXAMPLE MARKDOWN REPORT TEMPLATE -->
    <example-markdown-report>
        <![CDATA[
# ðŸ§ª Test Validation Report

## Executive Summary

**Test Health Score**: XX/100 (Status)

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Test Pass Rate | 95% | 100% | ðŸŸ¡ Good |
| Line Coverage | 78% | 80% | ðŸŸ¡ Close |
| Branch Coverage | 72% | 75% | ðŸŸ¡ Close |
| Function Coverage | 88% | 90% | ðŸŸ¡ Good |
| Flaky Tests | 2 | 0 | ðŸ”´ Needs Fix |

**Overall**: ðŸŸ¡ Good - Minor improvements needed

**Top Concerns**:
1. 2 flaky E2E tests detected
2. New Sidebar component missing tests
3. Coverage dropped 2% from baseline

---

## ðŸ”´ Existing Test Failures

| Test Name | Type | Location | Error | Root Cause | Priority | Fix |
|-----------|------|----------|-------|------------|----------|-----|
| UserAuth.testLogin | Unit | auth.test.tsx:42 | TypeError | Missing mock for Firebase | Critical | Add Firebase mock |
| Sidebar.navigation | E2E | sidebar.spec.ts:120 | Timeout | Toast blocks button | High | Dismiss toast first |

### Detailed Analysis

#### 1. UserAuth.testLogin (Critical)

**Error**:
```
TypeError: Cannot read property 'auth' of undefined
```

**Root Cause**: Firebase auth not properly mocked in test environment

**Recommended Fix**:
```typescript
// Add to test setup
vi.mock('@/lib/firebase', () => ({
  auth: {
    signInWithEmailAndPassword: vi.fn(),
  },
}));
```

---

## âš ï¸ Flaky Tests Detected

| Test | Flakiness Score | Indicators | Recommendation |
|------|-----------------|------------|----------------|
| sidebar.spec.ts::navigation | 40% | Timing-dependent | Add proper waits |
| profile.spec.ts::logout | 25% | Toast interference | Dismiss toast |

### Flakiness Analysis

**sidebar.spec.ts::navigation**:
- **Problem**: Test sometimes fails waiting for navigation
- **Root Cause**: Toast notification blocks navigation button
- **Fix**: Add `await page.getByText('Logged out').waitFor({ state: 'hidden' })`

---

## ðŸ“Š Coverage Analysis

### Current Coverage
- **Line Coverage**: 78% (+0.5% from baseline)
- **Branch Coverage**: 72% (+1.2% from baseline)
- **Function Coverage**: 88% (+2.1% from baseline)
- **Statement Coverage**: 78% (+0.5% from baseline)

### Coverage by Area

| Area | Coverage | Target | Gap |
|------|----------|--------|-----|
| Components | 82% | 80% | âœ… Met |
| Hooks | 75% | 80% | -5% |
| Utils | 95% | 90% | âœ… Exceeded |
| Pages | 65% | 80% | -15% âŒ |

### Critical Uncovered Code

1. **Sidebar.tsx** (0% coverage) - NEW FILE
   - Lines 45-120: Navigation logic
   - Lines 130-145: Close handler

2. **useAuth.ts** (68% coverage)
   - Lines 89-95: Error handling
   - Lines 102-108: Logout cleanup

---

## âœ… New Tests Recommended

### Unit Tests (6 recommended)

| Component/Function | Test Description | Priority | Effort | Impact |
|--------------------|------------------|----------|--------|--------|
| Sidebar.tsx | Should render navigation items | High | Low | High |
| Sidebar.tsx | Should toggle open/close | High | Low | High |
| Sidebar.tsx | Should highlight active route | Medium | Low | Medium |
| useAuth.ts | Should handle logout errors | High | Low | High |
| Navigation.tsx | Should integrate sidebar | Medium | Medium | Medium |
| utils/format.ts | Should format dates correctly | Low | Low | Low |

#### Recommended Test: Sidebar Navigation

```typescript
describe('Sidebar', () => {
  it('should render all navigation items', () => {
    render(<Sidebar isOpen={true} onClose={vi.fn()} />);
    
    expect(screen.getByText('Events')).toBeInTheDocument();
    expect(screen.getByText('Profile')).toBeInTheDocument();
    expect(screen.getByText('Settings')).toBeInTheDocument();
  });

  it('should call onClose when close button clicked', async () => {
    const onClose = vi.fn();
    render(<Sidebar isOpen={true} onClose={onClose} />);
    
    await userEvent.click(screen.getByLabelText('Close sidebar'));
    
    expect(onClose).toHaveBeenCalledTimes(1);
  });
});
```

### E2E Tests (3 recommended)

| User Flow | Test Description | Priority | Effort | Impact |
|-----------|------------------|----------|--------|--------|
| Navigation | User navigates via sidebar | High | Medium | High |
| Settings | User updates settings via sidebar | Medium | High | Medium |
| Mobile | Sidebar works on mobile | Medium | Medium | Medium |

#### Recommended Test: Sidebar Navigation Flow

```typescript
test('user navigates to settings via sidebar', async ({ page }) => {
  await page.goto('/');
  await page.getByTestId('sidebar-toggle').click();
  
  // Wait for sidebar animation
  await page.waitForTimeout(300);
  
  await page.getByTestId('sidebar-settings-link').click();
  
  await expect(page).toHaveURL('/settings');
  expect(await page.getByRole('heading', { name: 'Settings' })).toBeVisible();
});
```

---

## ðŸ“ˆ Test Quality Assessment

**Quality Score**: 82/100 (Good)

### Anti-Patterns Detected

| Pattern | Location | Impact | Recommendation |
|---------|----------|--------|----------------|
| Testing implementation | Button.test.tsx:45 | Medium | Test user behavior |
| Shallow assertions | Card.test.tsx:20 | Low | Add behavior assertions |
| Test interdependence | auth.test.tsx | High | Isolate tests |

### Quality Improvements

1. **Replace implementation testing**:
   ```typescript
   // âŒ Bad: Testing implementation
   expect(component.state.count).toBe(5);
   
   // âœ… Good: Testing behavior
   expect(screen.getByText('Count: 5')).toBeInTheDocument();
   ```

2. **Add comprehensive assertions**:
   ```typescript
   // âŒ Bad: Shallow
   expect(screen.getByRole('button')).toBeInTheDocument();
   
   // âœ… Good: Comprehensive
   const button = screen.getByRole('button', { name: 'Submit' });
   expect(button).toBeInTheDocument();
   expect(button).not.toBeDisabled();
   expect(button).toHaveClass('btn-primary');
   ```

---

## ðŸŽ¯ Prioritized Action Plan

### Immediate (Before Merge)
1. âœ… Fix UserAuth.testLogin critical failure
2. âœ… Add Sidebar unit tests (6 tests)
3. âœ… Stabilize flaky navigation test

### Short Term (This Sprint)
1. Add E2E test for sidebar navigation
2. Improve hooks coverage to 80%
3. Fix test interdependence issues

### Long Term (Future Improvements)
1. Add visual regression tests
2. Implement performance benchmarks
3. Add accessibility test suite

### Quick Wins
1. Add data-testid to new components (5 min)
2. Mock Firebase in failing tests (10 min)
3. Add toast dismissal in E2E (5 min)

---

## ðŸ“š Test Metrics History

| Date | Pass Rate | Coverage | Flaky Tests | Health Score |
|------|-----------|----------|-------------|--------------|
| Today | 95% | 78% | 2 | 82/100 |
| Last Week | 98% | 76% | 1 | 85/100 |
| Baseline | 100% | 75% | 0 | 88/100 |

**Trend**: ðŸ”´ Slight regression - needs attention

---

## ðŸŽ“ Testing Best Practices

### Recommended Reading
- [React Testing Library Best Practices](https://testing-library.com/docs/react-testing-library/intro/)
- [Playwright Best Practices](https://playwright.dev/docs/best-practices)
- [Testing Trophy](https://kentcdodds.com/blog/the-testing-trophy-and-testing-classifications)

### Project-Specific Guidelines
- Always use `data-testid` for test selectors
- Co-locate tests with components
- Test user behavior, not implementation
- Mock Firestore operations in unit tests
- Use real Firestore in E2E tests (with test data)

---

**Report generated**: [Timestamp]
**Tests executed**: [Count]
**Execution time**: [Duration]
        ]]>
    </example-markdown-report>

    <configuration>
        <test-execution>
            <mode>Strict</mode>
            <fail-on-error>true</fail-on-error>
            <parallel-execution>true</parallel-execution>
            <retry-flaky-tests>false</retry-flaky-tests>
            <timeout>30000</timeout>
        </test-execution>
        <reporting>
            <verbosity>Detailed</verbosity>
            <include-code-examples>true</include-code-examples>
            <include-metrics-history>true</include-metrics-history>
            <output-destinations>
                <destination>Console</destination>
                <destination>Markdown Report</destination>
                <destination>Coverage Report</destination>
            </output-destinations>
        </reporting>
        <coverage>
            <enabled>true</enabled>
            <reporter>text</reporter>
            <reporter>lcov</reporter>
            <reporter>html</reporter>
        </coverage>
    </configuration>
</testing-prompt>

